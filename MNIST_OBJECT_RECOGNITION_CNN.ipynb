{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_OBJECT_RECOGNITION_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCrI3akLRzLbx8aakSV4Rp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhareey/MNIST_OBJ_WITH_PYTORCH/blob/main/MNIST_OBJECT_RECOGNITION_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a03QEQ2L1App"
      },
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "# Dataset to be used is from torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axNw4g9b8B8n",
        "outputId": "11c9deb3-2d41-438d-93a4-f5518b6dada1"
      },
      "source": [
        "# Check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if train_on_gpu:\n",
        "  print('GPU ready, boss... Good to go...')\n",
        "else:\n",
        "  print('Sorry boss, u gonna have to do CPU...')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU ready, boss... Good to go...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0TG-lp99X7R"
      },
      "source": [
        "# Prepare the data to be used\n",
        "num_workers = 0\n",
        "batch_size = 20\n",
        "valid_size = 0.2\n",
        "transform = transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo0njifIVqEF",
        "outputId": "45733e7b-28c5-4f56-8ebc-11c6f10cf4d8"
      },
      "source": [
        "train_data = datasets.CIFAR10('data', \n",
        "                              train=True,\n",
        "                              download = True,\n",
        "                              transform = transform)\n",
        "test_data = datasets.CIFAR10('data',\n",
        "                             train = False,\n",
        "                             download= True,\n",
        "                             transform = transform)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJsp6KXsXAbE"
      },
      "source": [
        "# Get validaton data from train data\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size*num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr4MCnc-a31f"
      },
      "source": [
        "# Prepare dataloaders\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, sampler = train_sampler, num_workers= num_workers)\n",
        "valid_loader = DataLoader(train_data, batch_size=batch_size, sampler= valid_sampler, num_workers= num_workers)\n",
        "test_loader = DataLoader(test_data, batch_size = batch_size,num_workers= num_workers)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSzg6beteUZF"
      },
      "source": [
        "# Label Classes\n",
        "classes = ['airplane', 'automobile','bird', 'cat','deer','dog','frog', 'horse', 'ship','truck']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frLetxpZeo4A"
      },
      "source": [
        "# Define the Network Architecture\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    #Define the covnets layer\n",
        "    self.covnet1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "    self.covnet2 = nn.Conv2d(16, 32,3, padding=1)\n",
        "    self.covnet3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "    # Define the Fully connected layers\n",
        "    self.fc1 = nn.Linear(64*4*4, 500)\n",
        "    self.fc2 = nn.Linear(500,10)\n",
        "    # Define the Dropouts\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    # Define the maxpooling\n",
        "    self.mPool = nn.MaxPool2d(2,2)\n",
        "  \n",
        "  #Define the forward\n",
        "  def forward(self, x):\n",
        "    #print('Before covnet1, xshape=', x.shape)\n",
        "    x = self.mPool(F.relu(self.covnet1(x)))\n",
        "    #print('After covnet1, xshape=', x.shape)\n",
        "    x = self.mPool(F.relu(self.covnet2(x)))\n",
        "    #print('After covnet2, xshape=', x.shape)\n",
        "    x= self.mPool(F.relu(self.covnet3(x)))\n",
        "    #print('After covnet3, xshape=', x.shape)\n",
        "    x = x.view(-1, 64*4*4)\n",
        "    #print('After flattening, xshape=', x.shape)\n",
        "    x = self.dropout(x)\n",
        "    #print('After dropout, xshape=', x.shape)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    #print('After fc1, xshape=', x.shape)\n",
        "    x = self.dropout(x)\n",
        "    #print('After dropout2, xshape=', x.shape)\n",
        "    x = self.fc2(x)\n",
        "    #print('After fc2, xshape=', x.shape)\n",
        "    return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UJ-A27viOkk",
        "outputId": "edf2177e-5474-4575-d1b7-b2b479b8097b"
      },
      "source": [
        "# Instantiate the model\n",
        "model = Net()\n",
        "print(model)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (covnet1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (covnet2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (covnet3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=1024, out_features=500, bias=True)\n",
            "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (mPool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeW3L3DhigjU",
        "outputId": "d55606bd-0cef-4d7a-a545-b30cc6f73a9a"
      },
      "source": [
        "# Move model to gpu\n",
        "if train_on_gpu:\n",
        "  model.cuda()\n",
        "  print('Model moved to GPU, boss...')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model moved to GPU, boss...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVsDR8n6jAYA"
      },
      "source": [
        "# Set Model Optimizer and Loss Function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hPHv94fjdl2",
        "outputId": "5b84f2f5-a28f-4a74-9856-fea5225fcf40"
      },
      "source": [
        "# Training the model... Hold your breathe, Dare... This part could get messy\n",
        "n_epochs = 100\n",
        "valid_loss_min = np.Inf\n",
        "for epoch in range(1,n_epochs+1):\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "  # Set the model to train\n",
        "  model.train()\n",
        "  for data, target in train_loader:\n",
        "    # Move data to GPU\n",
        "    if train_on_gpu:\n",
        "      data, target = data.cuda(), target.cuda()\n",
        "      #print('Training Data Moved to GPU, bro....')\n",
        "    # Clear optimizer\n",
        "    optimizer.zero_grad()\n",
        "    # Run the forward pass\n",
        "    output = model(data)\n",
        "    # Calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # DO backward Propagation\n",
        "    loss.backward()\n",
        "    # Update Optimisation\n",
        "    optimizer.step()\n",
        "    #Update training Loss\n",
        "    train_loss += loss.item() * data.size(0)\n",
        "  \n",
        "  ##########################\n",
        "  #Validate the model\n",
        "  ##########################\n",
        "  # Set model to evaluate\n",
        "  model.eval()\n",
        "  for data, target in valid_loader:\n",
        "    # Moved data to cuda\n",
        "    if train_on_gpu:\n",
        "      data, target = data.cuda(), target.cuda()\n",
        "      #print('validation data Moved to GPu bro...')\n",
        "    # Run forward pass\n",
        "    output = model(data)\n",
        "    # Calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # Calc Validation Loss\n",
        "    valid_loss+= loss.item()* data.size(0)\n",
        "\n",
        "  # Calculate Average Losses\n",
        "  train_loss = train_loss/len(train_loader.sampler)\n",
        "  valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "  print('Epoch: {}\\t Training Loss: {:.6f}\\t Validation Loss:{:.6f}'.format(epoch, train_loss,valid_loss))\n",
        "\n",
        "  #Save model if model is better than previous\n",
        "  if valid_loss <= valid_loss_min:\n",
        "    print('Validation Loss Decreased from {:.6f} to {:.6f}. Saving Model...'.format(valid_loss_min, valid_loss))\n",
        "    torch.save(model.state_dict(),'model_cifar.pt')\n",
        "    valid_loss_min = valid_loss\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\t Training Loss: 0.645497\t Validation Loss:0.806181\n",
            "Validation Loss Decreased from inf to 0.806181. Saving Model...\n",
            "Epoch: 2\t Training Loss: 0.637656\t Validation Loss:0.799135\n",
            "Validation Loss Decreased from 0.806181 to 0.799135. Saving Model...\n",
            "Epoch: 3\t Training Loss: 0.631088\t Validation Loss:0.804042\n",
            "Epoch: 4\t Training Loss: 0.630410\t Validation Loss:0.792710\n",
            "Validation Loss Decreased from 0.799135 to 0.792710. Saving Model...\n",
            "Epoch: 5\t Training Loss: 0.623073\t Validation Loss:0.802110\n",
            "Epoch: 6\t Training Loss: 0.619559\t Validation Loss:0.792776\n",
            "Epoch: 7\t Training Loss: 0.611428\t Validation Loss:0.799146\n",
            "Epoch: 8\t Training Loss: 0.603045\t Validation Loss:0.798630\n",
            "Epoch: 9\t Training Loss: 0.598675\t Validation Loss:0.795339\n",
            "Epoch: 10\t Training Loss: 0.597919\t Validation Loss:0.795425\n",
            "Epoch: 11\t Training Loss: 0.586808\t Validation Loss:0.794617\n",
            "Epoch: 12\t Training Loss: 0.587282\t Validation Loss:0.791163\n",
            "Validation Loss Decreased from 0.792710 to 0.791163. Saving Model...\n",
            "Epoch: 13\t Training Loss: 0.576755\t Validation Loss:0.798470\n",
            "Epoch: 14\t Training Loss: 0.573936\t Validation Loss:0.795319\n",
            "Epoch: 15\t Training Loss: 0.568325\t Validation Loss:0.785730\n",
            "Validation Loss Decreased from 0.791163 to 0.785730. Saving Model...\n",
            "Epoch: 16\t Training Loss: 0.561614\t Validation Loss:0.788623\n",
            "Epoch: 17\t Training Loss: 0.558957\t Validation Loss:0.789421\n",
            "Epoch: 18\t Training Loss: 0.554561\t Validation Loss:0.782515\n",
            "Validation Loss Decreased from 0.785730 to 0.782515. Saving Model...\n",
            "Epoch: 19\t Training Loss: 0.545794\t Validation Loss:0.787055\n",
            "Epoch: 20\t Training Loss: 0.542794\t Validation Loss:0.789464\n",
            "Epoch: 21\t Training Loss: 0.533907\t Validation Loss:0.792503\n",
            "Epoch: 22\t Training Loss: 0.529766\t Validation Loss:0.787151\n",
            "Epoch: 23\t Training Loss: 0.529057\t Validation Loss:0.781313\n",
            "Validation Loss Decreased from 0.782515 to 0.781313. Saving Model...\n",
            "Epoch: 24\t Training Loss: 0.520834\t Validation Loss:0.775832\n",
            "Validation Loss Decreased from 0.781313 to 0.775832. Saving Model...\n",
            "Epoch: 25\t Training Loss: 0.513193\t Validation Loss:0.800494\n",
            "Epoch: 26\t Training Loss: 0.508785\t Validation Loss:0.784073\n",
            "Epoch: 27\t Training Loss: 0.510028\t Validation Loss:0.776826\n",
            "Epoch: 28\t Training Loss: 0.499294\t Validation Loss:0.785535\n",
            "Epoch: 29\t Training Loss: 0.493396\t Validation Loss:0.786447\n",
            "Epoch: 30\t Training Loss: 0.493435\t Validation Loss:0.790157\n",
            "Epoch: 31\t Training Loss: 0.488196\t Validation Loss:0.779492\n",
            "Epoch: 32\t Training Loss: 0.478552\t Validation Loss:0.785727\n",
            "Epoch: 33\t Training Loss: 0.474339\t Validation Loss:0.787660\n",
            "Epoch: 34\t Training Loss: 0.473379\t Validation Loss:0.784134\n",
            "Epoch: 35\t Training Loss: 0.462779\t Validation Loss:0.783347\n",
            "Epoch: 36\t Training Loss: 0.463160\t Validation Loss:0.786281\n",
            "Epoch: 37\t Training Loss: 0.456580\t Validation Loss:0.782999\n",
            "Epoch: 38\t Training Loss: 0.454115\t Validation Loss:0.783383\n",
            "Epoch: 39\t Training Loss: 0.446793\t Validation Loss:0.786488\n",
            "Epoch: 40\t Training Loss: 0.441688\t Validation Loss:0.778801\n",
            "Epoch: 41\t Training Loss: 0.434820\t Validation Loss:0.787901\n",
            "Epoch: 42\t Training Loss: 0.430808\t Validation Loss:0.782158\n",
            "Epoch: 43\t Training Loss: 0.423732\t Validation Loss:0.788673\n",
            "Epoch: 44\t Training Loss: 0.423920\t Validation Loss:0.781999\n",
            "Epoch: 45\t Training Loss: 0.417786\t Validation Loss:0.788431\n",
            "Epoch: 46\t Training Loss: 0.411561\t Validation Loss:0.786232\n",
            "Epoch: 47\t Training Loss: 0.409422\t Validation Loss:0.796244\n",
            "Epoch: 48\t Training Loss: 0.407215\t Validation Loss:0.789286\n",
            "Epoch: 49\t Training Loss: 0.405340\t Validation Loss:0.788191\n",
            "Epoch: 50\t Training Loss: 0.393575\t Validation Loss:0.791331\n",
            "Epoch: 51\t Training Loss: 0.391724\t Validation Loss:0.802578\n",
            "Epoch: 52\t Training Loss: 0.385616\t Validation Loss:0.792173\n",
            "Epoch: 53\t Training Loss: 0.378922\t Validation Loss:0.794561\n",
            "Epoch: 54\t Training Loss: 0.376685\t Validation Loss:0.798092\n",
            "Epoch: 55\t Training Loss: 0.378215\t Validation Loss:0.797390\n",
            "Epoch: 56\t Training Loss: 0.367398\t Validation Loss:0.800239\n",
            "Epoch: 57\t Training Loss: 0.360556\t Validation Loss:0.798947\n",
            "Epoch: 58\t Training Loss: 0.363564\t Validation Loss:0.806425\n",
            "Epoch: 59\t Training Loss: 0.361023\t Validation Loss:0.804154\n",
            "Epoch: 60\t Training Loss: 0.349970\t Validation Loss:0.801019\n",
            "Epoch: 61\t Training Loss: 0.347658\t Validation Loss:0.806065\n",
            "Epoch: 62\t Training Loss: 0.345617\t Validation Loss:0.808042\n",
            "Epoch: 63\t Training Loss: 0.339230\t Validation Loss:0.803399\n",
            "Epoch: 64\t Training Loss: 0.338630\t Validation Loss:0.807904\n",
            "Epoch: 65\t Training Loss: 0.333153\t Validation Loss:0.799787\n",
            "Epoch: 66\t Training Loss: 0.327857\t Validation Loss:0.812558\n",
            "Epoch: 67\t Training Loss: 0.324799\t Validation Loss:0.811665\n",
            "Epoch: 68\t Training Loss: 0.321625\t Validation Loss:0.810352\n",
            "Epoch: 69\t Training Loss: 0.323768\t Validation Loss:0.811739\n",
            "Epoch: 70\t Training Loss: 0.313470\t Validation Loss:0.825854\n",
            "Epoch: 71\t Training Loss: 0.308988\t Validation Loss:0.823200\n",
            "Epoch: 72\t Training Loss: 0.305718\t Validation Loss:0.818319\n",
            "Epoch: 73\t Training Loss: 0.304762\t Validation Loss:0.827741\n",
            "Epoch: 74\t Training Loss: 0.300188\t Validation Loss:0.824383\n",
            "Epoch: 75\t Training Loss: 0.295739\t Validation Loss:0.825385\n",
            "Epoch: 76\t Training Loss: 0.293036\t Validation Loss:0.822350\n",
            "Epoch: 77\t Training Loss: 0.290104\t Validation Loss:0.827147\n",
            "Epoch: 78\t Training Loss: 0.288782\t Validation Loss:0.819804\n",
            "Epoch: 79\t Training Loss: 0.282121\t Validation Loss:0.834009\n",
            "Epoch: 80\t Training Loss: 0.281442\t Validation Loss:0.833028\n",
            "Epoch: 81\t Training Loss: 0.285105\t Validation Loss:0.839293\n",
            "Epoch: 82\t Training Loss: 0.278426\t Validation Loss:0.834096\n",
            "Epoch: 83\t Training Loss: 0.268757\t Validation Loss:0.840601\n",
            "Epoch: 84\t Training Loss: 0.268275\t Validation Loss:0.836049\n",
            "Epoch: 85\t Training Loss: 0.262618\t Validation Loss:0.855424\n",
            "Epoch: 86\t Training Loss: 0.267452\t Validation Loss:0.855317\n",
            "Epoch: 87\t Training Loss: 0.261183\t Validation Loss:0.837626\n",
            "Epoch: 88\t Training Loss: 0.259236\t Validation Loss:0.843261\n",
            "Epoch: 89\t Training Loss: 0.252506\t Validation Loss:0.848104\n",
            "Epoch: 90\t Training Loss: 0.249805\t Validation Loss:0.858085\n",
            "Epoch: 91\t Training Loss: 0.251914\t Validation Loss:0.848214\n",
            "Epoch: 92\t Training Loss: 0.241052\t Validation Loss:0.865511\n",
            "Epoch: 93\t Training Loss: 0.240580\t Validation Loss:0.866794\n",
            "Epoch: 94\t Training Loss: 0.236197\t Validation Loss:0.859139\n",
            "Epoch: 95\t Training Loss: 0.237477\t Validation Loss:0.858515\n",
            "Epoch: 96\t Training Loss: 0.239173\t Validation Loss:0.862317\n",
            "Epoch: 97\t Training Loss: 0.231789\t Validation Loss:0.862675\n",
            "Epoch: 98\t Training Loss: 0.226563\t Validation Loss:0.885632\n",
            "Epoch: 99\t Training Loss: 0.230380\t Validation Loss:0.868070\n",
            "Epoch: 100\t Training Loss: 0.226954\t Validation Loss:0.868521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5M-VKL4kH2G"
      },
      "source": [
        "# Looks like the model will be better if it trains longer but im testing it anyway\n",
        "# Load the saved model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVU_nR3EDX6X",
        "outputId": "be6a37b8-a77b-4fbd-d8a2-0f530108540c"
      },
      "source": [
        "model.load_state_dict(torch.load('model_cifar.pt'))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6fe58OlH7Lv"
      },
      "source": [
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "# Set the model to evaluate\n",
        "model.eval()\n",
        "for data, target in test_loader:\n",
        "  if train_on_gpu:\n",
        "    data, target = data.cuda(), target.cuda()\n",
        "  output = model(data)\n",
        "  loss = criterion(output, target)\n",
        "  test_loss += loss.item() * data.size(0)\n",
        "  _, pred = torch.max(output, 1)\n",
        "  correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "  correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "  for i in range(batch_size):\n",
        "    label = target.data[i]\n",
        "    class_correct[label] += correct[i].item()\n",
        "    class_total[label] += 1"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6Dngr4tdC9c",
        "outputId": "1f22e9c5-bdf7-41e5-be39-9e936b96598f"
      },
      "source": [
        "# Calculating Average Test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.771920\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi6GPAzQdo3q",
        "outputId": "3fb1730b-ceb1-4776-d4a3-f2e89bc43fca"
      },
      "source": [
        "# Test Loss = 0.95\n",
        "# \n",
        "for i in range(len(classes)):\n",
        "  if class_total[i]> 0:\n",
        "    print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (classes[i], 100*class_correct[i]/class_total[i],np.sum(class_correct[i]),np.sum(class_total[i])))\n",
        "  else:\n",
        "    print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "print('\\n Test Accuracy (Overall): %2d%% (%2d/%2d)' % (100. * np.sum(class_correct)/np.sum(class_total), np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy of airplane: 81% (812/1000)\n",
            "Test Accuracy of automobile: 85% (854/1000)\n",
            "Test Accuracy of  bird: 61% (617/1000)\n",
            "Test Accuracy of   cat: 52% (528/1000)\n",
            "Test Accuracy of  deer: 63% (634/1000)\n",
            "Test Accuracy of   dog: 63% (632/1000)\n",
            "Test Accuracy of  frog: 82% (824/1000)\n",
            "Test Accuracy of horse: 79% (790/1000)\n",
            "Test Accuracy of  ship: 85% (852/1000)\n",
            "Test Accuracy of truck: 79% (794/1000)\n",
            "\n",
            " Test Accuracy (Overall): 73% (7337/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxq2_rXxgADi"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}